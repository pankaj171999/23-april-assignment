{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6816d1a-4edb-4beb-bb57-2c1549ba9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1(Ans)In machine learning, high-dimensional data refers to data with a large number of features or variables. \n",
    "The curse of dimensionality is a common problem in machine learning, where the performance of the model deteriorates as the number of features increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191fee5-4c49-47f5-be8b-31d1f4882fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2(Ans)As the dimensionality increases, the number of data points required for good performance of any machine learning algorithm increases exponentially. \n",
    "The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0b64a-acf5-4bdd-9406-33105f7ddfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3(Ans)As the number of dimensions or features increases, the amount of data needed to generalize the machine learning model accurately increases exponentially. \n",
    "The increase in dimensions makes the data sparse, and it increases the difficulty of generalizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2519be-f3f4-4140-95a5-68979ee0e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4(Ans)While both methods are used for reducing the number of features in a dataset, there is an important difference. \n",
    "Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad7955-a254-4fe6-bc6d-b32ed24099c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5(Ans)Disadvantages Of Dimensionality Reduction\n",
    "\n",
    "* We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "* It may need a lot of processing power.\n",
    "* Interpreting transformed characteristics might be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dea2a5-99ef-4db8-9889-f65cd48d04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6(Ans)Because of this inherent sparsity we end up overfitting, when we add more features to our data, which means we need more data to avoid sparsity â€” \n",
    "and that the curse of dimensionality: as the number of features increase, our data become sparser, which results in overfitting, and we therefore need more data to avoid it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033bcfc-0ed2-4afc-9548-5e0e38e397fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7(Ans)In such situations, you can set a threshold value and use the missing values ratio method. The higher the threshold value, the more aggressive will be the \n",
    "dimensionality reduction. If the percentage of missing values in a variable exceeds the threshold, you can drop the variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
